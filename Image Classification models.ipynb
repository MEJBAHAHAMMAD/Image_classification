{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>\n",
    "            <h1> Image Classification models </h1>\n",
    "        </title>\n",
    "        <body>\n",
    "            <p> For Building <b> Image Classification models </b> we will go over the following options: <p><br/>\n",
    "            <li> Training a small network from scratch (as a baseline ) </li>\n",
    "            <li> Using the bottleneck features of a pre-trained network </li>\n",
    "            <li> Fine-tuning the top layers of pre-trained network </li>\n",
    "        </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <h1><b>Data pre-processing and Data Augmentation<b></h1>      \n",
    "    </head>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    #rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('dogs-vs-cats/train/cat.0.jpg')\n",
    "# convert the image to numpy array data\n",
    "x = img_to_array(image)\n",
    "# this is a numpy array with shape (1, 3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "i = 0\n",
    "for batch in data_generator.flow(x, batch_size = 1,\n",
    "                                save_to_dir='preview', save_prefix='cat',\n",
    "                                save_format='jpeg'):\n",
    "    i+=1\n",
    "    if i > 20:\n",
    "        break # Break. Because of, Otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <h1><b> Trainint a small convent from scratch : 80% accuracy </b></h1>\n",
    "    </head>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimension our image datasets\n",
    "# image_width, image_height = 150, 150\n",
    "\n",
    "# train_data_dir = 'dogs-vs-cats/train'\n",
    "# validation_data_dir = 'dogs-vs-cats/test1'\n",
    "\n",
    "# np_train_sampels = 2000\n",
    "# np_test_samples = 800\n",
    "\n",
    "# epochs = 50\n",
    "# batch_size = 16\n",
    "\n",
    "\n",
    "# if k.image_data_format() == 'channels_first':\n",
    "#     input_shape = (3, image_width, image_height)\n",
    "# else:\n",
    "#     input_shape = (image_width, image_height, 3)\n",
    "    \n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape = input_shape))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3,3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss = 'binary_crossentropy',\n",
    "#              optimizer = 'rmsprop',\n",
    "#              matrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "#                                   shear_range=0.2,\n",
    "#                                   zoom_range=0.2,\n",
    "#                                   horizontal_flip=True)\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directiory( train_data_dir,\n",
    "#                                                  target_size = (image_width, image_height),\n",
    "#                                                  batch_size= batch_size,\n",
    "#                                                  class_mode = 'binary')\n",
    "\n",
    "\n",
    "# validation_generatior = test_datagen.flow_from_directiory( validation_data_dir, \n",
    "#                                                          target_size = (image_width, image_height),\n",
    "#                                                          batch_size = batch_size,\n",
    "#                                                          class_mode='binary')\n",
    "\n",
    "# model.fit_generator(train_generator, \n",
    "#                    steps_per_epoch = np_train_sampels//batch_size,\n",
    "#                    epochs = epochs,\n",
    "#                    validation_data = validation_generatior,\n",
    "#                    validation_steps =np_test_samples//batch_size)\n",
    "\n",
    "\n",
    "# model.save_weights('first_try.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13496 images belonging to 2 classes.\n",
      "Found 11504 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 89s 713ms/step - loss: 0.7122 - accuracy: 0.5140 - val_loss: 0.6767 - val_accuracy: 0.6325\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 86s 688ms/step - loss: 0.6703 - accuracy: 0.6290 - val_loss: 0.6373 - val_accuracy: 0.6587\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 86s 685ms/step - loss: 0.6356 - accuracy: 0.6365 - val_loss: 0.6162 - val_accuracy: 0.6600\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 86s 691ms/step - loss: 0.6155 - accuracy: 0.6645 - val_loss: 0.6339 - val_accuracy: 0.6438\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 87s 698ms/step - loss: 0.6146 - accuracy: 0.6780 - val_loss: 0.6094 - val_accuracy: 0.6825\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 87s 692ms/step - loss: 0.6209 - accuracy: 0.6835 - val_loss: 0.5375 - val_accuracy: 0.7225\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 86s 692ms/step - loss: 0.6161 - accuracy: 0.6790 - val_loss: 0.5514 - val_accuracy: 0.7013\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 87s 699ms/step - loss: 0.5863 - accuracy: 0.6855 - val_loss: 0.5711 - val_accuracy: 0.6913\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 87s 693ms/step - loss: 0.5866 - accuracy: 0.6965 - val_loss: 0.5222 - val_accuracy: 0.7312\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 87s 692ms/step - loss: 0.5870 - accuracy: 0.6915 - val_loss: 0.5255 - val_accuracy: 0.7387\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 87s 693ms/step - loss: 0.5881 - accuracy: 0.7100 - val_loss: 0.5394 - val_accuracy: 0.7312\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 87s 693ms/step - loss: 0.5806 - accuracy: 0.7100 - val_loss: 0.5009 - val_accuracy: 0.7550\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 87s 695ms/step - loss: 0.5687 - accuracy: 0.7140 - val_loss: 0.5448 - val_accuracy: 0.7100\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 86s 691ms/step - loss: 0.5575 - accuracy: 0.7279 - val_loss: 0.5045 - val_accuracy: 0.7475\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 87s 696ms/step - loss: 0.5465 - accuracy: 0.7315 - val_loss: 0.5194 - val_accuracy: 0.7375\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 87s 698ms/step - loss: 0.5391 - accuracy: 0.7274 - val_loss: 0.5106 - val_accuracy: 0.7513\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 87s 696ms/step - loss: 0.5566 - accuracy: 0.7240 - val_loss: 0.5060 - val_accuracy: 0.7650\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 87s 699ms/step - loss: 0.5457 - accuracy: 0.7425 - val_loss: 0.5434 - val_accuracy: 0.7462\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 87s 694ms/step - loss: 0.5456 - accuracy: 0.7290 - val_loss: 0.5446 - val_accuracy: 0.7462\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 87s 697ms/step - loss: 0.5315 - accuracy: 0.7495 - val_loss: 0.5108 - val_accuracy: 0.7575\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 89s 714ms/step - loss: 0.5187 - accuracy: 0.7635 - val_loss: 0.5985 - val_accuracy: 0.7275\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 88s 701ms/step - loss: 0.5135 - accuracy: 0.7735 - val_loss: 0.5142 - val_accuracy: 0.7650\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 88s 706ms/step - loss: 0.5087 - accuracy: 0.7535 - val_loss: 0.4421 - val_accuracy: 0.7962\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 88s 703ms/step - loss: 0.5216 - accuracy: 0.7540 - val_loss: 0.4575 - val_accuracy: 0.7800\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 87s 698ms/step - loss: 0.5088 - accuracy: 0.7615 - val_loss: 0.4981 - val_accuracy: 0.7775\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 87s 698ms/step - loss: 0.4908 - accuracy: 0.7775 - val_loss: 0.4662 - val_accuracy: 0.7812\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 87s 693ms/step - loss: 0.5000 - accuracy: 0.7660 - val_loss: 0.4681 - val_accuracy: 0.7887\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 89s 712ms/step - loss: 0.4976 - accuracy: 0.7800 - val_loss: 0.5110 - val_accuracy: 0.7738\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 89s 712ms/step - loss: 0.5027 - accuracy: 0.7710 - val_loss: 0.4556 - val_accuracy: 0.7975\n",
      "Epoch 30/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4921 - accuracy: 0.7782"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "#model = Sequential()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3),input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
